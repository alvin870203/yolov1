{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect ImageNet2012 torchvision Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref for data preparation: https://github.com/pytorch/examples/blob/main/imagenet/extract_ILSVRC.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = torchvision.datasets.ImageNet(root='../data/imagenet2012/', split='train', transform=torchvision.transforms.ToTensor())\n",
    "dataset_val = torchvision.datasets.ImageNet(root='../data/imagenet2012/', split='val', transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of train dataset: 1281167\n",
      "Len of val dataset: 50000\n",
      "Number of classes: 1000\n",
      "\n",
      "First 10th classes\n",
      "Class 0: ('tench', 'Tinca tinca'), ID=0\n",
      "Class 1: ('goldfish', 'Carassius auratus'), ID=1\n",
      "Class 2: ('great white shark', 'white shark', 'man-eater', 'man-eating shark', 'Carcharodon carcharias'), ID=2\n",
      "Class 3: ('tiger shark', 'Galeocerdo cuvieri'), ID=3\n",
      "Class 4: ('hammerhead', 'hammerhead shark'), ID=4\n",
      "Class 5: ('electric ray', 'crampfish', 'numbfish', 'torpedo'), ID=5\n",
      "Class 6: ('stingray',), ID=6\n",
      "Class 7: ('cock',), ID=7\n",
      "Class 8: ('hen',), ID=8\n",
      "Class 9: ('ostrich', 'Struthio camelus'), ID=9\n",
      "\n",
      "Sample\n",
      "Image: tensor([[[0.1216, 0.1412, 0.1373,  ..., 0.2667, 0.3137, 0.3059],\n",
      "         [0.1373, 0.1529, 0.1529,  ..., 0.2431, 0.2314, 0.2471],\n",
      "         [0.1529, 0.1529, 0.1451,  ..., 0.1804, 0.2314, 0.2118],\n",
      "         ...,\n",
      "         [0.3137, 0.2706, 0.2549,  ..., 0.4196, 0.3020, 0.1333],\n",
      "         [0.3176, 0.2902, 0.2667,  ..., 0.4157, 0.2980, 0.1333],\n",
      "         [0.2902, 0.3137, 0.2941,  ..., 0.4039, 0.2902, 0.1294]],\n",
      "\n",
      "        [[0.1216, 0.1412, 0.1373,  ..., 0.2902, 0.3373, 0.3294],\n",
      "         [0.1373, 0.1529, 0.1529,  ..., 0.2902, 0.2784, 0.2941],\n",
      "         [0.1529, 0.1529, 0.1451,  ..., 0.2353, 0.2863, 0.2667],\n",
      "         ...,\n",
      "         [0.3725, 0.3294, 0.3137,  ..., 0.4510, 0.3137, 0.1333],\n",
      "         [0.3765, 0.3490, 0.3255,  ..., 0.4471, 0.3098, 0.1333],\n",
      "         [0.3451, 0.3647, 0.3529,  ..., 0.4314, 0.3020, 0.1294]],\n",
      "\n",
      "        [[0.1529, 0.1725, 0.1686,  ..., 0.2431, 0.2902, 0.2824],\n",
      "         [0.1686, 0.1843, 0.1843,  ..., 0.2431, 0.2235, 0.2392],\n",
      "         [0.1843, 0.1843, 0.1765,  ..., 0.1843, 0.2353, 0.2078],\n",
      "         ...,\n",
      "         [0.2824, 0.2471, 0.2392,  ..., 0.3686, 0.2784, 0.1333],\n",
      "         [0.2863, 0.2667, 0.2510,  ..., 0.3647, 0.2745, 0.1412],\n",
      "         [0.2549, 0.2863, 0.2784,  ..., 0.3608, 0.2745, 0.1373]]]) (type(x)=<class 'torch.Tensor'>), hw=<built-in method size of Tensor object at 0x7f04c62c49a0>\n",
      "Label: 0 (type(y)=<class 'int'>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Len of train dataset: {len(dataset_train)}\")\n",
    "print(f\"Len of val dataset: {len(dataset_val)}\")\n",
    "print(f\"Number of classes: {len(dataset_train.classes)}\")\n",
    "print(f\"\\nFirst 10th classes\")\n",
    "for i in range(10):\n",
    "    print(f\"Class {i}: {dataset_train.classes[i]}, ID={dataset_train.class_to_idx[dataset_train.classes[i][0]]}\")\n",
    "print(f\"\\nSample\")\n",
    "x, y = dataset_train[2]\n",
    "print(f\"Image: {x} ({type(x)=}), hw={x.size}\")  # PIL Image with non-fixed size\n",
    "print(f\"Label: {y} ({type(y)=})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example imgs batch: torch.Size([1, 3, 250, 250])\n",
      "Example labels batch: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=1, shuffle=False)\n",
    "print(f\"Example imgs batch: {next(iter(dataloader_train))[0].size()}\")\n",
    "print(f\"Example labels batch: {next(iter(dataloader_train))[1].size()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nir-rppg-benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
